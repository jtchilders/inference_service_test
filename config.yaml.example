# Sample configuration file for the agentic workflow
# Copy this file to config.yaml and modify with your settings

# Sophia LLM service configuration
sophia:
  url: "https://data-portal-dev.cels.anl.gov/resource_server/sophia/vllm/v1"

# Aurora PBS job configuration
# Aurora uses MFA via mobile app, not SSH keys
# SSH connections use ControlMaster to reuse existing authenticated sessions
aurora:
  host: "aurora"
  user: "parton"  # Set via environment variable
  # SSH ControlMaster configuration for Aurora authentication
  control_master: true
  pbs_template: "jobs/train_cifar100.pbs.template"
  queue: "debug"

# Globus Compute configuration (alternative to direct PBS)
# Requires setting up a Globus Compute endpoint on Aurora
globus_compute:
  enabled: false  # Set to true to use Globus Compute instead of direct PBS
  endpoint_id: "your-aurora-endpoint-id"  # Set after endpoint registration
  endpoint_config_path: "config/globus_endpoint.yaml"
  # Authentication method for Globus Compute
  # Options: "native_client", "confidential_client", "client_credentials"
  auth_method: "native_client"
  # Optional: specify functions timeout and other execution parameters
  function_timeout: 3600  # 1 hour timeout for training functions
  max_retries: 2

# Repository configuration for deployment
# If not specified, will use current git repository
repository:
  url: "https://github.com/jtchilders/inference_service_test.git"  # Optional: auto-detected from current repo
  branch: "main"  # Optional: auto-detected from current branch

# Working directory configuration
# Hierarchical structure: {working_dir_base}/{launch_iteration_base}/{job_base}
working_dirs:
  # Base directory on Aurora for all workflow runs
  aurora_base: "/lus/flare/projects/datascience/parton/workflows"
  # Base directory for local results and logs
  local_base: "results"
  # Directory naming patterns
  launch_iteration_pattern: "{timestamp}_{experiment_id}"  # e.g., "20241201_143022_exp_001"
  job_pattern: "job_{job_id}"  # e.g., "job_exp_001_iter_0"

# Data and results configuration
data:
  dir: "data/cifar-100-python"

results:
  dir: "results"

# Workflow configuration
polling_interval: 60  # seconds between job status checks


# Optional: Model and training defaults
# training:
#   default_model_type: "resnet18"
#   default_batch_size: 128
#   default_learning_rate: 0.001
#   default_num_epochs: 50

# Optional: Logging configuration
# logging:
#   level: "INFO"
#   file: "logs/agent_workflow.log"
#   max_size: "100MB"
#   backup_count: 5 